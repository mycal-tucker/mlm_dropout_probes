dataset:
  observation_fieldnames:
     - index
     - sentence
     - lemma_sentence
     - upos_sentence
     - xpos_sentence
     - morph
     - head_indices
     - governance_relations
     - secondary_relations
     - extra_info
     - embeddings
  corpus:
    root: data/conj/
    train_path: text.conllx
    dev_path: text.conllx
    test_path: text.conllx
  embeddings:
    type: token
    root: data/conj/
    train_path: text.hdf5
    dev_path: text.hdf5
    test_path: text.hdf5
    break_on_qmark: False
  batch_size: 512  # How many counterfactuals do you want to update at once?
model:
  hidden_dim: 768
  model_type: BERT-disk
  use_disk: True
  model_layer: -1  # Doesn't matter, gets overwritten by script that iterates over layers.
probe:
  task_signature: word # word, word_pair
  task_name: parse-depth  # parse-depth, parse-distance
  num_layers: 3
  maximum_rank: 32
  diagonal: False
  params_path: predictor.params
probe_training:
  epochs: 0  # Ignored in counterfactual generation
  loss: L1
reporting:
  root: dropout4_depth_3layer/model_depth
  observation_paths:
    train_path: train.observations
    dev_path: dev.observations
    test_path: test.observations
  prediction_paths:
    train_path: train.predictions
    dev_path: dev.predictions
    test_path: test.predictions
